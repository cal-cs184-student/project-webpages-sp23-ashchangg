<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>[CS 184] Project 3-1 </title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-opaquegray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="ca7650b8-f069-4fad-9dcd-6f0bb35630b2" class="page sans"><header><h1 class="page-title">[CS 184] Project 3-1 </h1></header><div class="page-body"><h2 id="e4bfe59f-d595-46d9-8e68-44cfd81ce0ac" class=""><mark class="highlight-yellow_background"><strong>Overview</strong></mark></h2><p id="561190dd-f247-4a34-b523-6845399b3f90" class="">In this project we implemented the following:</p><ol type="1" id="be74706a-2d39-4a07-8db0-07ed85f2fb50" class="numbered-list" start="1"><li><a href="https://www.notion.so/CS-184-Project-3-1-ca7650b8f0694fad9dcd6f0bb35630b2">Ray Generation and Scene Intersection</a></li></ol><ol type="1" id="6cc86772-0e19-4fc0-9edb-6df1bea0754e" class="numbered-list" start="2"><li><a href="https://www.notion.so/CS-184-Project-3-1-ca7650b8f0694fad9dcd6f0bb35630b2">Bounding Volume Hierarchy</a></li></ol><ol type="1" id="5dc1e8fa-8eb0-4323-b1b0-dd706b9c03eb" class="numbered-list" start="3"><li><a href="https://www.notion.so/CS-184-Project-3-1-ca7650b8f0694fad9dcd6f0bb35630b2">Direct Illumination</a></li></ol><ol type="1" id="201351d6-90b5-493d-9e6e-9ee7fbc58297" class="numbered-list" start="4"><li><a href="https://www.notion.so/CS-184-Project-3-1-ca7650b8f0694fad9dcd6f0bb35630b2">Global Illumination</a></li></ol><ol type="1" id="f5f15814-1a30-455d-908c-04f53889696e" class="numbered-list" start="5"><li><a href="https://www.notion.so/CS-184-Project-3-1-ca7650b8f0694fad9dcd6f0bb35630b2">Adaptive Sampling</a></li></ol><p id="9cb8b311-ceee-49ad-88a4-bf116ea4d17c" class="">Throughout the project, we ran into a few segfaults and errors relating to previous portions of the project. We solved these by taking a step back to understand the problem (rereading the spec and understanding our code) and searching through given resources (Ed, lecture slides, etc).</p><h2 id="0f914f3c-9bd0-4928-a5ad-16c9edf8c864" class=""><mark class="highlight-yellow_background"><strong>Part 1: Ray Generation and Scene Intersection</strong></mark></h2><p id="fca9e618-9ba3-4835-ad92-22f2a216d331" class="">In this part, we generate rays and implement ray intersection testing (ray-triangle and ray-sphere intersection). </p><h3 id="61a8f3fd-1da3-49f1-b50f-8a1c67092476" class=""><strong><em>Ray Generation</em></strong></h3><p id="a49b2bbb-d93f-4225-8606-7ed9128a2f64" class=""><em>W</em>e first transform the given image coordinates to camera space, generate the ray in the camera space, then transform this ray into the world space. We are told that <code>(0, 0)</code> (bottom left corner) in the image space corresponds to <code>(-tan(0.5 x hFov), -tan(0.5 x vFov), -1)</code> and <code>(1, 1)</code> (top right corner) corresponds to <code>(tan(0.5 x hFov), tan(0.5 x vFov), -1)</code> where <code>hFov</code> and <code>vFov</code> are field of view angles along the X and Y axis. Given this, we can transform image coordinates to camera space by computing the following x and y coordinates:</p><figure class="block-color-gray_background callout" style="white-space:pre-wrap;display:flex" id="bfa473d0-b86e-4494-a17e-a2aa85030857"><div style="font-size:1.5em"><span class="icon">üí°</span></div><div style="width:100%"><code>double x_coord = (x * 2 * tan(0.5 * hFov * (PI / 180))) + (-1 * tan(0.5 * hFov * (PI / 180)));
double y_coord = (y * 2 * tan(0.5 * vFov * (PI / 180))) + (-1 * tan(0.5 * vFov * (PI / 180)));
Vector3D camera_point = Vector3D(x_coord, y_coord, -1);</code></div></figure><p id="496e898f-f350-4ab6-9d76-94b6aa8c9187" class="">From here, we create a ray in the camera space and transform it to world space. We do this by setting the origin of our ray to be <code>pos</code> (the camera position in the world space) and direction to be equal to our calculated camera_point transformed back to world space <code>c2w * camera_point</code> where <code>c2w</code> is the camera-to-world rotation matrix.</p><p id="4c578270-bfbd-458f-96a5-cf8e74d7c85b" class="">Now that we have our ray generated, we want to update our <code>raytrace_pixel</code> function to estimate the integral of radiance over a given pixel coordinate <code>(x, y)</code>. To do this, we generate <code>num_samples</code> random camera rays, estimate the scene radiance along that ray (which we get from the above method) by calling <code>PathTracer::est_radiance_global_illumination(Ray r)</code>, and returning the average radiance of these samples. </p><h3 id="74377792-a49c-47e1-801c-4f06aad9f4db" class=""><em><strong>Primitive Intersection </strong></em></h3><p id="579fadbc-4bab-4d1a-af55-fc668e4259a9" class="">Next, we want to implement ray intersection tests to see whether there is a valid intersection between a primitive and the input ray. We will also report the location of the nearest intersection point and populate the input <code>Intersection *isect</code> structure.</p><p id="1f5d619e-d37c-4c54-b57e-51e5b3d8b631" class="">In order to test for ray-triangle intersection, we implement the two functions <code>has_intersection</code> and <code>intersect</code> in <code>Triangle</code>. The <code>has_intersection</code> method returns <code>true</code> if there is a valid intersection. We determine this by checking for ray-plane intersection (image below from lecture slides), then checking if the hit point is inside the triangle similar to what we did in Assignment 1. The intersection is only valid if it occurs at <code>t</code> that lies within <code>min_t</code> and <code>max_t</code> of the input ray. If this is true, we want to update <code>max_t</code> accordingly so that we are tracking the nearest intersection (we can ignore all future intersections that are farther away). Finally, we implement <code>intersect</code> which will update the <code>Intersection</code> data accordingly if there is a valid intersection.</p><p id="624e8ac6-4368-4751-a220-3ddc812722ac" class="">Similarly, we can test whether there is a valid intersection between a sphere and the input ray using the following calculations below. Since a sphere can have two resulting <code>t</code> values, we will select the closest intersection and update <code>max_t</code> accordingly.</p><div id="92251a15-9043-4878-a6a2-afcde0311d27" class="column-list"><div id="fb0ae473-b846-4ab4-8f1d-7e9dd52075da" style="width:50%" class="column"><figure id="1ffe616c-7941-4fc2-9e97-9cacbe4d304a" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.06.26_PM.png"><img style="width:384px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.06.26_PM.png"/></a><figcaption>Ray-Plane Intersection Calculations</figcaption></figure></div><div id="886678f5-a707-416f-96c2-b17d0632b292" style="width:50%" class="column"><figure id="c361ba83-fd59-4a09-b074-076d860c55dd" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.11.17_PM.png"><img style="width:384px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.11.17_PM.png"/></a><figcaption>Ray-Sphere Intersection Calculations</figcaption></figure></div></div><h3 id="6724ba0b-764a-4663-ad23-ab390cfaf0e2" class=""><em>Results</em></h3><p id="0b7ee40e-8394-489c-ae22-740e6e1a3850" class="">Here are some images with normal shading:</p><div id="e4f919ff-6d2d-41a1-9d9c-3046229a515a" class="column-list"><div id="62019821-7cc1-4f0d-885d-8b766dd88420" style="width:50%" class="column"><figure id="9f774a4a-ae37-4642-9fd1-d6eb933632c8" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBempty.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBempty.png"/></a><figcaption><code>CBEmpty.dae</code></figcaption></figure></div><div id="246b1f42-c26b-41bf-80df-797f6c91fd7e" style="width:50.000000000000014%" class="column"><figure id="f88dc6f7-4bc4-4317-9156-410474e7f4a2" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBspheres.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBspheres.png"/></a><figcaption><code>CBspheres_lambertian.dae</code></figcaption></figure></div></div><h2 id="d4e12fc6-a124-41b6-bea6-5ab68857dc43" class=""><mark class="highlight-yellow_background"><strong>Part 2: Bounding Volume Hierarchy</strong></mark></h2><p id="9c001687-31d1-4e53-bf69-c4fb005b3ccb" class="">In this part we implement bounding volume hierarchy, starting with constructing the BVH, checking for bounding box intersections, then checking for BVH intersections. </p><h3 id="d156ec5c-cee0-45ca-8488-9d870e929459" class=""><em><strong>BVH Construction</strong></em></h3><p id="fc4affd6-c911-452b-ac23-912faaab0ede" class="">Our BVH structure is as follows (outlined in lecture slides):</p><ul id="2c872c4b-1506-48fc-a731-d548c7f404dc" class="bulleted-list"><li style="list-style-type:disc">Internal nodes store a bounding box and references to child nodes</li></ul><ul id="0b204cd4-34cb-4c3b-bceb-070118309680" class="bulleted-list"><li style="list-style-type:disc">Leaf nodes store a bounding box and list of objects</li></ul><ul id="88ee3fc1-7ede-4c7f-9b6a-13abe13392ff" class="bulleted-list"><li style="list-style-type:disc">Nodes represent a subset of primitives in the scene (all objects in the subtree)</li></ul><p id="daa9c9f2-2bb4-4d04-bf76-e3933eb75ab3" class="">We start by computing the bounding box of a list of primitives and initializing a new BVHNode using that bounding box. We will then keep dividing the primitives into two subsets (‚Äùleft‚Äù and ‚Äúright‚Äù) by computing a split point along the axis that gives us the most benefit based on our heuristic (median) and using the <code>partition</code> method. The subsets will be assigned to the current node‚Äôs left and right children will be. This step will be repeated recursively until there are no more than <code>max_leaf_size</code> primitives in the list (at this point we also want to update the node‚Äôs <code>start</code> and <code>end</code> iterators as the current <code>start</code> and <code>end</code>.</p><h3 id="b0398a40-9645-4334-b8ca-49a16fa3120d" class=""><em><strong>BVH Intersection</strong></em></h3><p id="0acaaed4-e29f-41d3-9d7e-aa235f4121b5" class="">Before we can implement our BVH intersect method, we need to first implement bounding box intersection tests. To do this, we first calculate our t values along each axis as follows</p><figure id="85925b08-a99b-48ff-9aa5-52bdd44c03c0" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.55.18_PM.png"><img style="width:192px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_7.55.18_PM.png"/></a></figure><p id="8a006458-b312-4794-87ce-b1026e3b0524" class="">We will then set our t interval as the maximum of our t values calculated from the min corner of the bounding box, to the minimum of our t values calculated from the max corner of the bounding box. If this t interval is valid, the intersection is valid.</p><p id="e182e25c-534b-4eb0-8eb2-23757427dc8a" class="">To check for BVH intersection, we perform the following steps</p><ul id="753ace42-61af-4694-b340-222fdc6a3e46" class="bulleted-list"><li style="list-style-type:disc">If the ray misses the bounding box of the given BVH node, we return <code>false</code></li></ul><ul id="4ded77b7-3e47-4a73-8277-ffa47b9ccaa6" class="bulleted-list"><li style="list-style-type:disc">Otherwise:<ul id="406dfa55-e5b8-4b15-901f-80ffcb5309ff" class="bulleted-list"><li style="list-style-type:circle">If the given BVH node is a leaf node, we will test intersection with all objects and return the closest intersection, and return <code>true</code></li></ul><ul id="7278e1b0-0ca1-490a-ba61-1abb2fc70e26" class="bulleted-list"><li style="list-style-type:circle">If the given BVH node is not a leaf node, we will take the closer intersection of the node‚Äôs left and right children</li></ul></li></ul><h3 id="ea9ce14c-2352-4298-8a61-7f1ffe3044a8" class=""><em>Results</em></h3><p id="032538e7-4005-42c9-8d23-7f08e1bc495d" class="">Normal shading for a few large¬†<em>.dae</em>¬†files rendered with BVH acceleration:</p><div id="5a1b7a32-f9f5-49e2-9306-a27fd3ae61e2" class="column-list"><div id="a005b889-db94-4fab-8405-10949f5ea44f" style="width:49.99999999999999%" class="column"><figure id="f8f5ef82-74df-44a3-8207-7c73b2a3410a" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/maxplanck.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/maxplanck.png"/></a><figcaption><code><em>maxplanck.dae</em></code></figcaption></figure></div><div id="f6bd2c92-59a0-4c3b-95c8-134b0bf8856a" style="width:49.99999999999999%" class="column"><figure id="5a4362aa-d6db-4462-b503-d51c44cf5c7c" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/cow.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/cow.png"/></a><figcaption><code><em>cow.dae</em></code></figcaption></figure></div></div><div id="3bf59036-dcd7-42c2-99db-dce3f4575183" class="column-list"><div id="f37d1233-a0a3-46a1-a322-1580903086b1" style="width:50%" class="column"><figure id="0ec1dcd0-223f-4cea-b54a-02996d467c1e" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBlucy.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBlucy.png"/></a><figcaption><code>CBlucy</code><code><em>.dae</em></code></figcaption></figure></div><div id="4ebdb1fc-2ebe-4320-bf61-d59105dff733" style="width:50%" class="column"><figure id="6493592d-a3db-45a4-8eb3-5eac004e9553" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/test_beast2.png"><img style="width:800px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/test_beast2.png"/></a><figcaption><code><code><code><code><code>beast</code></code></code></code></code><code><em>.dae</em></code></figcaption></figure></div></div><h3 id="f36065ae-e04f-4102-8bd0-026344208d42" class=""><em>Comparing Rendering Times</em></h3><p id="b5cc74e5-d906-4180-a0b2-8306e749904d" class="">For scenes with moderately complex geometries, we can see drastic improvements in rendering times with the help of BVH acceleration. This is because BVH allows us to traverse the scene and discard sets of primitives that a particular ray is guaranteed to not intersect for more efficiency.</p><div id="5904530a-06db-478f-af32-af0e80ee62be" class="column-list"><div id="c1aa47d2-5a8e-4136-8490-c83b382bbf7a" style="width:50%" class="column"><p id="10a68563-255b-4bae-81d8-04724a45f52a" class=""><code><em>maxplanck.dae</em></code></p><ul id="f4cf140b-c59f-4bc2-9bed-4be8693f41af" class="bulleted-list"><li style="list-style-type:disc">Before: 7m 40s</li></ul><ul id="8cc1b9f5-ae74-4904-8c00-f4195dcb388a" class="bulleted-list"><li style="list-style-type:disc">After: 0.1277s</li></ul></div><div id="36d266a7-6c01-48f7-904b-c6b73e1c9051" style="width:50%" class="column"><p id="02e2ece9-1fe9-45de-a9d8-bf7bfec883e2" class=""><code><em>cow.dae</em></code></p><ul id="f889e9af-0359-47e5-be9a-8d9f476059b3" class="bulleted-list"><li style="list-style-type:disc">Before: 47s</li></ul><ul id="1d1ab52f-7ff9-4c9e-af29-8f95dffeb5ff" class="bulleted-list"><li style="list-style-type:disc">After: 0.1055 sec</li></ul></div></div><div id="c1a13c6d-f58d-47d3-b580-5e4dc3a848dc" class="column-list"><div id="3bc43cb0-19f7-4e6b-93d0-cd5f7508fc3e" style="width:50%" class="column"><p id="65271448-bed2-49f0-9362-42476e805190" class=""><code>CBLucy</code><code><em>.dae</em></code></p><ul id="a78d2bc2-dad9-4875-9fd2-6bd4249ea0fe" class="bulleted-list"><li style="list-style-type:disc">Before: 20m 50s</li></ul><ul id="562408f5-252d-420c-8be8-814e3f6f9077" class="bulleted-list"><li style="list-style-type:disc">After: 0.0733s</li></ul></div><div id="86d4b82c-eedf-4122-bf43-9002ff6f634d" style="width:50%" class="column"><p id="3518a92d-38d8-4c67-a250-8e951343776f" class=""><code><code><code><code><code>beast</code></code></code></code></code><code><em>.dae</em></code></p><ul id="6ab22081-e421-4838-ba18-8a857d1196b2" class="bulleted-list"><li style="list-style-type:disc">Before: 9m 9s</li></ul><ul id="4c847c7d-ebfc-4ef0-9db0-595f6331666d" class="bulleted-list"><li style="list-style-type:disc">After: 0.0821s</li></ul></div></div><h2 id="269889f1-04e7-4046-aa29-5130c570bf59" class=""><mark class="highlight-yellow_background"><strong>Part 3: Direct Illumination</strong></mark></h2><p id="c2427ca6-526e-4bcb-b270-6c6c389979a6" class="">In this part of the project, we implement two methods of calculating direct illumination ‚Äî uniform hemisphere sampling and importance sampling lights.</p><h3 id="1dbc9ee9-acdb-4aa2-8126-f6f9700ed007" class=""><em>Uniform Hemisphere Sampling (</em><code><em>estimate_direct_lighting_hemisphere</em></code><em>)</em></h3><p id="d4bd3d6b-d3c4-4d1b-a11a-8e381cf00417" class="">In the uniform hemisphere sampling method, we estimate the direct lighting on a point by sampling uniformly in a hemisphere. First we cast a ray from the camera through a specific pixel into the scene. Once we detect an intersection, we will calculate how much light is reflected back towards the camera at this intersection point, helping us determine the color of the corresponding pixel. This is done by estimating how much light has arrived at that intersection point, which we do by integrating over all the light arriving in a hemisphere around the hit point using the Monte Carlo estimator (image below).</p><figure id="d0b870c5-c93f-4e35-8026-353b3be47a74" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Untitled.png"><img style="width:567px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Untitled.png"/></a></figure><p id="62169870-7834-4c1c-bfec-7bf5f90ce58f" class="">Breakdown of the equation above in our own terms:</p><figure id="7521ae22-e710-4780-baaf-34e9a8bd616c" class="image" style="text-align:left"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.44.37_PM.png"><img style="width:174px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.44.37_PM.png"/></a></figure><ul id="16c04774-3eda-41ed-a4bb-a6e3081d2591" class="bulleted-list"><li style="list-style-type:disc"><code>DiffuseBSDF::f</code><ul id="0a198594-96c0-43df-8936-b574eb7941a3" class="bulleted-list"><li style="list-style-type:circle">Input: <code>wo</code> (outgoing light direction in local space of point of intersection), <code>wi</code> (incident light direction in local space of point of intersection); in this case <code>wo</code> is <code>w_out</code> as provided in the starter code and <code>wi</code> is a sampled incoming ray direction in the hemisphere converted to object space</li></ul><ul id="542f9016-df29-4cde-b4a0-a44876eb5788" class="bulleted-list"><li style="list-style-type:circle">Returns: reflectance in the given incident/outgoing directions (we can simply return <code>reflectance / PI</code> where <code>reflectance</code> is the ‚Äúalbedo‚Äù of a surface or the range of total absorption vs total reflection per color channel)</li></ul></li></ul><figure id="cfe3d5e0-945c-48d0-a7dc-5c091ccfe8eb" class="image" style="text-align:left"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.49.44_PM.png"><img style="width:105px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.49.44_PM.png"/></a></figure><ul id="4d9897fb-99a1-41fb-b9c6-5e3225d9d164" class="bulleted-list"><li style="list-style-type:disc">Calculated using <code>intersection.bsdf-&gt;get_emission()</code> where intersection is the <code>Intersection</code> struct we get from testing if the ray with origin <code>hit_p</code> and direction <code>w_i</code> intersects the BVH</li></ul><figure id="2b49de47-1f69-4e86-9ae0-74536e791ab4" class="image" style="text-align:left"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.53.04_PM.png"><img style="width:74px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.53.04_PM.png"/></a></figure><ul id="5265069f-3149-4397-8331-3db224c3ff79" class="bulleted-list"><li style="list-style-type:disc">Calculated using the given <code>cos_theta</code> of <code>wi</code> (sampled incoming ray direction in the hemisphere) converted to object space</li></ul><figure id="4512c280-2623-4721-84f5-2e0a2a413f80" class="image" style="text-align:left"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.56.38_PM.png"><img style="width:74px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/Screen_Shot_2023-03-17_at_9.56.38_PM.png"/></a></figure><ul id="2bc1eae2-7b9a-438d-886e-c3590094bdf2" class="bulleted-list"><li style="list-style-type:disc"><code>1 / (2PI)</code> in this case, because we sample uniformly.</li></ul><h3 id="2061cb90-ac80-476c-8fbe-83c2c3dcdab7" class=""><em><strong><strong>Importance Sampling Lights</strong></strong></em><em> (</em><code><em>estimate_direct_lighting_importance</em></code><em>)</em></h3><p id="6e876354-bf58-452a-888b-8249fb7321be" class="">In our implementation of the <code><strong>estimate_direct_lighting_importance</strong></code> function, we aimed to estimate the lighting from the intersection coming directly from a light using importance sampling. Instead of sampling uniformly in a hemisphere, we sampled mainly from the lights in the scene.</p><p id="cb8bc09b-7055-4171-a42a-ce73b4188cdc" class="">We initialized <code><strong>L_out</strong></code> to store the final result. We looped through all the lights in the scene, and for each light, we initialized the number of samples to 1 by default. We then checked if the current light was not a delta light, and if so, we set the number of samples to the value of <code><strong>ns_area_light</strong></code>.</p><p id="c9bc284c-3d2e-47d7-9edf-5eac6ca5aa4a" class="">Next, we looped through the samples and called the <code><strong>sample_L</strong></code> function of the light to obtain the incident radiance <code><strong>L_i</strong></code>, the incident direction <code><strong>w_i</strong></code>, the distance to the light <code><strong>distToLight</strong></code>, and the probability density function value <code><strong>pdf</strong></code>. We created a new ray with the origin at the hit point and the direction towards the light, and set its minimum and maximum intersection distances to <code>EPS_F</code> and <code>distToLight - EPS_F</code> , respectively.</p><p id="337a190a-1a5c-4360-8dec-49e9359d2b0a" class="">We calculated the BRDF value <code><strong>f_r</strong></code> for the outgoing direction <code><strong>w_out</strong></code> and the incoming direction <code><strong>w_i</strong></code> transformed into the local coordinate system. We also computed the cosine term <code><strong>cos_j</strong></code> for the incoming direction.</p><p id="4d63a0bc-43e3-4c85-a0ab-7db6264a5814" class="">We checked if the new ray intersects with the scene geometry using the <code><strong>bvh-&gt;intersect</strong></code> function. If there is an intersection, we skipped the current sample iteration. Otherwise, we accumulated the radiance contribution <code><strong>L_sample</strong></code> by multiplying the product of the BRDF, the incident radiance, the cosine term, and the inverse of the <code><strong>pdf</strong></code>.</p><p id="eb9722a6-1d5e-43b1-aef3-f7ca42502c8d" class="">After processing all the samples, we added the average radiance contribution <code><strong>L_sample / num_samples</strong></code> to the final radiance <code><strong>L_out</strong></code>. We repeated this process for all the lights in the scene.</p><p id="82438202-9372-4a36-a731-91e83b3c3ebf" class="">Finally, we returned the accumulated radiance <code><strong>L_out</strong></code>, which represents the direct lighting estimation at the intersection point using importance sampling.</p><h3 id="26b86675-c6c1-4b4a-87c1-1f0ba04dd8b1" class=""><em>Results - Uniform Hemisphere Sampling vs Light Sampling</em></h3><p id="0e97d61e-6cae-4bca-8ec9-e2766632ffe0" class="">Below are renders of <code>CBLucy.dae</code> and <code>CBbunny.dae</code> using uniform hemisphere sampling and light sampling. </p><div id="bf936053-9c3d-4954-ac37-dff73a415854" class="column-list"><div id="f5d70c14-9795-4559-871c-3e9373cca6a6" style="width:50%" class="column"><figure id="f0690d11-0629-4300-8cda-544875137fc2" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/uniform_lucy.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/uniform_lucy.png"/></a><figcaption>Uniform Hemisphere Sampling on <code>CBLucy.dae</code></figcaption></figure><figure id="c41a88de-f1d7-4846-a150-a2ce33211dfa" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_bunny.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_bunny.png"/></a><figcaption>Uniform Hemisphere Sampling on <code>CBbunny.dae</code></figcaption></figure><p id="b4afa981-5583-488f-8b3f-69f0fe892c2e" class="">
</p></div><div id="c1f9a040-79f4-4e89-ac86-ee01b6d76176" style="width:50%" class="column"><figure id="3130578d-2c8d-45b6-a382-19b740f78c0b" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_lucy.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_lucy.png"/></a><figcaption>Light Sampling on <code>CBLucy.dae</code></figcaption></figure><figure id="09decf8a-d290-43b7-9e32-40f03ed6106f" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_bunny%201.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/light_bunny%201.png"/></a><figcaption>Light Sampling on <code>CBbunny.dae</code></figcaption></figure></div></div><p id="1d7c6f55-05b7-4555-9250-cb7e478f5d33" class="">Above we can see that light sampling produces better results than uniform hemisphere sampling as it focuses on sampling directions towards light sources, which contribute more significantly to the final radiance, thus reducing noise and converging faster to the correct solution with fewer samples. As seen in the above images, the light sampled image is much smoother.</p><h3 id="8a94cfa2-d4f9-4446-9b1e-054ea6b81fd9" class=""><em>Results - Comparing Noise Levels with Light Sampling</em></h3><p id="91e37169-d74b-44ff-81ab-3bc9c1f0ea10" class="">Below are renders of <code>CBbunny.dae</code> with using light sampling with different <code>l</code> flags (1, 4, 16, 64) and 1 sample per pixel. We can see that as we increase the light rays, the noise levels visibly decrease, and we get a smoother/less noisy outputs.</p><div id="ed8e0efe-08d1-4508-a89a-671cb0e4120e" class="column-list"><div id="c1fb84bb-a3ca-467e-897b-9e7768133ed5" style="width:50%" class="column"><figure id="b2aee22a-6b54-4dec-bf18-b19253ef5677" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L1.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L1.png"/></a><figcaption>L = 1</figcaption></figure></div><div id="3e907b44-9370-4c86-b6b5-94fd35a8b8e6" style="width:50%" class="column"><figure id="313551e5-f2b2-468a-a03a-4bb378b35c86" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L4.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L4.png"/></a><figcaption>L = 4</figcaption></figure></div></div><div id="51de6322-f29c-4b7f-a264-80ba8939ef16" class="column-list"><div id="3d4ea6d9-05d9-49b5-933c-5987e935f23b" style="width:50%" class="column"><figure id="dcbc2896-0c7b-4cf6-b50b-edbe77142fbe" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L16.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L16.png"/></a><figcaption>L = 16</figcaption></figure></div><div id="9418d664-e6d1-4b68-b7d8-f4738b101574" style="width:50%" class="column"><figure id="1e5194d2-c136-4910-a59b-3b3aa6468806" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L64.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_L64.png"/></a><figcaption>L = 64</figcaption></figure></div></div><h2 id="daa752fc-fa0d-49d5-9b87-cac63fa53a20" class=""><mark class="highlight-yellow_background"><strong>Part 4: Global Illumination</strong></mark></h2><p id="1bd8e4db-aee9-4f2a-a60f-fb9d09056dec" class="">In our implementation of the indirect lighting function, we first initialize <code>L_out</code> to zero. </p><p id="9d3f9308-31d6-456c-93d4-9394c9c42ea0" class="">Next, we call the <code><strong>one_bounce_radiance</strong></code> function to compute the direct lighting component of the radiance at the intersection point and add the result to <code><strong>L_out</strong></code>. We use Russian roulette to decide whether to continue path tracing or terminate it. The variable <code><strong>cpdf</strong></code> represents the probability of continuing the path tracing. If our coin flip with this probability is not successful, we return the current value of <code><strong>L_out</strong></code>.</p><p id="c37aec41-bae1-44c5-8cf9-c8b8c1a0704e" class="">If the Russian roulette test passes, we proceed to sample an incoming direction <code><strong>w_i</strong></code> using the <code><strong>sample_f</strong></code> method of the BSDF at the intersection point. We then create a new ray <code><strong>new_ray</strong></code> with the sampled direction and the intersection point and set its depth to the current ray&#x27;s depth minus 1.</p><p id="27e4860b-3fdd-4bbf-b8f2-3c28433e8e8c" class="">We test the new ray for intersection with the scene geometry using the <code><strong>bvh-&gt;intersect</strong></code> method. If there&#x27;s an intersection, we compute the indirect lighting contribution recursively by calling <code><strong>at_least_one_bounce_radiance</strong></code> with the new ray and intersection. We add the indirect lighting contribution to <code><strong>L_out</strong></code>, using the reflectance equation. If we applied Russian roulette, we also divide the contribution by <code><strong>1.0 - cpdf</strong></code>.</p><p id="7faa2553-181b-4cc1-b165-6c0fc1d817a3" class="">Finally, we return the accumulated radiance <code><strong>L_out</strong></code>.</p><h3 id="b8f27ecc-0ad7-43af-9ac4-01b509663bdb" class=""><em>Results</em></h3><p id="9d2c873c-5f57-4391-897d-39ccdd2b189d" class="">Below are some images rendered with global (direct and indirect) illumination using 1024 samples per pixel.</p><div id="b10a8cb3-5f52-4268-b002-c038b68fa28f" class="column-list"><div id="f02f194b-9a14-4a0e-86f7-05d3f46f2bc7" style="width:50%" class="column"><figure id="3730df2a-1e5a-43b9-bed1-0d11b0f4b21c" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_lucyhg_sample_1024.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_lucyhg_sample_1024.png"/></a><figcaption>lucy_hg rendered at 1024 samples/pixel</figcaption></figure></div><div id="84b4c51e-a70d-424b-9eeb-e8f5302dca9c" style="width:50%" class="column"><figure id="4d45d3dd-6b23-4309-acb5-da43a5a6e0c1" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_100.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_100.png"/></a><figcaption>CBbunny rendered at 1024 samples/pixel</figcaption></figure></div></div><p id="0057aa72-95e5-442c-a2c4-5bf965587e2c" class="">
</p><p id="472a1fdf-ad29-4303-833d-ad3e1a6b2ceb" class="">Below are examples of <code>CBspheres_lambertian.dae</code> rendered with only direct illumination, then only indirect illumination, using 1024 samples per pixel.</p><div id="849db2b1-5e1c-406c-b960-b54ac289c367" class="column-list"><div id="5a833ba9-65db-4482-b23f-218df7f557af" style="width:50.000000000000014%" class="column"><figure id="8c9a155b-c3c8-48f7-86ec-59b6caa94473" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/direct_spheres.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/direct_spheres.png"/></a><figcaption><code>CBspheres_lambertian.dae</code> rendered with only direct illumination</figcaption></figure></div><div id="c2483879-3551-451a-9aff-f58a3a30cf9a" style="width:50%" class="column"><figure id="e7b1545d-685f-407f-979e-cac32ed01d3e" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/direct_spheres%201.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/direct_spheres%201.png"/></a><figcaption><code>CBspheres_lambertian.dae</code> rendered with only indirect illumination</figcaption></figure></div></div><p id="c313b834-7a65-4170-84ac-a5ab24011241" class="">
</p><p id="2f452623-83b6-4d24-9a88-e4e0d11b4c51" class="">Below are examples of <code>CBbunny.dae</code> rendered at different values of <code>max_ray_depth</code>. </p><div id="9d6a0859-4a23-41d1-99ed-ea681d951386" class="column-list"><div id="376172d2-cdbf-4a24-9328-b11a511918c0" style="width:50%" class="column"><figure id="24383b2e-fff1-451c-8c2e-4cdfa3b537d5" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_16_8.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/CBbunny_16_8.png"/></a><figcaption>max_ray_depth = 0</figcaption></figure></div><div id="11cb3d07-0842-4cc1-8602-b1c9efb40351" style="width:50%" class="column"><figure id="193675e9-584f-49dc-9af6-d6a48a1423bb" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_1.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_1.png"/></a><figcaption>max_ray_depth = 1</figcaption></figure></div></div><p id="b3319ad0-ab74-49b7-9ae5-8e41dc9ced0b" class="">
</p><div id="ff585c4c-1e0d-4538-b0f0-aa9f7ed2065a" class="column-list"><div id="786bccba-2168-4a1c-aaa8-59640128f058" style="width:50%" class="column"><figure id="1d369000-9d8b-44a0-a854-a06b4ab7ce4e" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_2.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_2.png"/></a><figcaption>max_ray_depth = 2</figcaption></figure></div><div id="89fbfa99-5ac8-41d6-a158-ab35c4e746f1" style="width:50%" class="column"><figure id="ed84454d-1cb0-4909-bcfb-2be7522a53d4" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_3.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_3.png"/></a><figcaption>max_ray_depth = 3</figcaption></figure></div></div><figure id="8ca818c8-6426-4db2-b3ac-8ccc3aa9e8ca" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_100.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_maxd_100.png"/></a><figcaption>max_ray_depth = 100</figcaption></figure><p id="011d801c-10f0-4e48-ab0a-923f3808b8e7" class="">
</p><p id="03147286-9ed2-4bca-8596-9cb1c8463f96" class="">Below is a comparison of the <code>bench.dae</code> file at 1, 2, 4, 8, 16, 64, and 1024 samples per pixel. As we can see, it gets considerably smoother and less noisy with more samples per pixel taken.</p><div id="0bd073fc-e7a1-4b4d-b03c-0c7df901c9a2" class="column-list"><div id="3443ff86-16ee-4a54-a99e-0e1186f78281" style="width:50%" class="column"><figure id="0ee99039-4256-4f25-854a-ed32e23cfc5d" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_1.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_1.png"/></a><figcaption>1 sample per pixel</figcaption></figure></div><div id="d5a69679-5157-4c0f-a309-1b7aa85b205f" style="width:50%" class="column"><figure id="eec4fefc-5228-4ca0-9894-ed2ffc633246" class="image" style="text-align:center"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_2.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_2.png"/></a><figcaption>2 samples per pixel</figcaption></figure><p id="26230c08-d852-4c3c-86be-07dc356201f9" class="">
</p></div></div><div id="f589f2de-6b6b-4d37-8756-6276495954fb" class="column-list"><div id="078fd665-6ae6-4264-b458-00c1cc0130da" style="width:50%" class="column"><figure id="987c6f05-a528-4598-9101-17307a3701a5" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_4.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_4.png"/></a><figcaption>4 samples per pixel</figcaption></figure></div><div id="b5a1d7e4-37f9-4e82-b1c7-b42ce26f3fc8" style="width:50%" class="column"><figure id="7e89eec1-8650-4b90-bb77-3d739b6e1dc6" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_8.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_8.png"/></a><figcaption>8 samples per pixel</figcaption></figure></div></div><p id="a9107840-00ef-4046-8bf8-e65f2b7455f4" class="">
</p><div id="5bdd7081-87c7-432c-a138-db232b8ec21b" class="column-list"><div id="ea070642-ea84-4090-9318-680f72bb46e6" style="width:50%" class="column"><figure id="aba54178-2d31-4a00-bdfc-31a234a2e43c" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_16.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_16.png"/></a><figcaption>16 samples per pixel</figcaption></figure></div><div id="6ab08363-e8c6-4571-938c-9f27a4ea8dac" style="width:50%" class="column"><figure id="bc38d362-8988-4453-a8a0-8ce69f38188a" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_64.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_64.png"/></a><figcaption>64 samples per pixel</figcaption></figure></div></div><figure id="16db8683-e4e7-4baf-aac6-938eccbf1cf5" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_1024.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p4_wr_bench_sample_1024.png"/></a><figcaption>1024 samples per pixel</figcaption></figure><h2 id="c613db2b-3d65-4e13-9780-c9d617bb22f7" class=""><mark class="highlight-yellow_background"><strong>Part 5: Adaptive Sampling</strong></mark></h2><p id="4e0b68a9-a3ff-449f-b6b4-e4955e189f5f" class="">In our implementation of adaptive sampling for part 5, we modified the <code><strong>raytrace_pixel</strong></code> function.</p><p id="815e48c0-0df1-4ffe-912e-c857809911db" class="">First, we initialized the required variables, including the total number of samples (<code><strong>num_samples</strong></code>) and the pixel&#x27;s bottom-left corner (<code><strong>origin</strong></code>). </p><p id="51316cca-5299-4702-a85d-dd72be81e126" class="">After the initial raytracing from Part 1.2, in order to do adaptive sampling, the first thing we did was to update the values of <code><strong>s1</strong></code> and <code><strong>s2</strong></code> by adding the luminance of the radiance <code><strong>c</strong></code> and the square of the luminance, respectively.</p><p id="04df3fd0-79d8-4290-9583-31e418930df4" class="">We checked if the current iteration <code><strong>i</strong></code> is a multiple of <code><strong>samplesPerBatch</strong></code>. If so, we computed the mean and variance of the luminance values and calculated the confidence interval <code><strong>i_val</strong></code>. We compared <code><strong>i_val</strong></code> with the product of <code><strong>maxTolerance</strong></code> and the mean. If the confidence interval was less than or equal to the product, we stopped the loop.</p><p id="02e19f91-bf8e-4755-a578-878ba2f31b0d" class="">Finally, we computed the average radiance by dividing the accumulated radiance <code><strong>sum</strong></code> by the total number of samples <code><strong>i</strong></code>. We updated the sample buffer and the sample count buffer with the average radiance and the total number of samples, respectively.</p><p id="ead0a15d-67d6-4564-8bde-a509fd4fbd16" class="">In addition, we have an <code><strong>autofocus</strong></code> function that adjusts the camera&#x27;s focal distance based on the intersection of a generated ray with the scene. We create a ray <code><strong>r</strong></code> using the <code><strong>camera-&gt;generate_ray()</strong></code> function with the provided <code><strong>loc</strong></code> values normalized by the sample buffer width and height. We then find the intersection with the scene using the <code><strong>bvh-&gt;intersect()</strong></code> function and update the camera&#x27;s focal distance with the intersection distance <code><strong>isect.t</strong></code>.</p><p id="2f3c0dff-a3a8-40f2-9c71-396c88947edb" class="">
</p><p id="b7d95a2d-5d52-4054-b791-2a1f3d9a9e49" class="">Below are two examples of images rendered with adaptive sampling. </p><div id="14f4c77c-1ee5-4a3f-8b18-f0400e0d00b0" class="column-list"><div id="4dd4b464-e9e7-48f6-bd1f-7b10b80bb6f2" style="width:50%" class="column"><figure id="a425a6de-1c66-4dc1-8c2c-8f8186e061fe" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/part5_bunny.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/part5_bunny.png"/></a></figure></div><div id="54053485-a658-4d50-b50f-a09ee14e8f5d" style="width:50%" class="column"><figure id="9d7345ee-8f64-483e-9732-617c3013fb39" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/part5_bunny_rate.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/part5_bunny_rate.png"/></a></figure></div></div><p id="394a2c72-5d12-47af-be0e-76a2bfaa8b5b" class="">
</p><div id="407ce39b-9a8e-4972-92d9-00fb48f12b41" class="column-list"><div id="696c4562-f41a-44dd-8c94-b8939988f7b4" style="width:50%" class="column"><figure id="164f481b-beba-4e83-84a4-ae38a69a1fd9" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p5_wr_lucy_hg_2048.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p5_wr_lucy_hg_2048.png"/></a></figure></div><div id="6ca5f524-5dcb-4267-a194-0b727a6316db" style="width:50%" class="column"><figure id="eae50c57-9b62-4a49-93c8-12e8d9eb3c89" class="image"><a href="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p5_wr_lucy_hg_2048_rate.png"><img style="width:480px" src="%5BCS%20184%5D%20Project%203-1%20ca7650b8f0694fad9dcd6f0bb35630b2/p5_wr_lucy_hg_2048_rate.png"/></a></figure></div></div><h2 id="5bf4e598-d588-4be7-83e2-dca334694088" class=""><mark class="highlight-yellow_background">Conclusion</mark></h2><p id="ba158eef-0830-4d80-a023-8157dc1c1afa" class="">For this project, we worked through each section together, switching off who would be typing and who would be guiding with pseudocode. Because all the parts were so intertwined, we ran into many bugs throughout the project and oftentimes found ourselves searched through code from previous portions. While we struggled a lot with bugs in the project, our method of collaboration was helpful so that we could both understand the project as a whole and more easily find helpful spots for debugging. Overall, we learned a lot about ray/path tracing and how to render images with different types of shading more efficiently.</p><h3 id="6dd36111-587d-4fe9-85ad-23b7ff0a6d08" class=""><em><strong>Project Webpage</strong></em></h3><p id="84aff105-4af3-4a82-8f8f-6126a756782a" class=""><a href="https://cal-cs184-student.github.io/project-webpages-sp23-ashchangg/proj3-1/index.html">https://cal-cs184-student.github.io/project-webpages-sp23-ashchangg/proj3-1/index.html</a></p><p id="5c9e412d-3483-40e7-85ef-5b212d9c98ba" class="">
</p></div></article></body></html>